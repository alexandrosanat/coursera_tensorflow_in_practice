{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow_datasets.core.utils import gcs_utils\n",
    "import numpy as np\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_utils.gcs_dataset_info_files = lambda *args, **kwargs: None\n",
    "gcs_utils.is_dataset_on_gcs = lambda *args, **kwargs: False\n",
    "\n",
    "# Import IMDB dataset\n",
    "imdb, _ = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)\n",
    "\n",
    "# Split test/training sets\n",
    "train_data, test_data = imdb[\"train\"], imdb[\"test\"]\n",
    "\n",
    "training_sentences = []\n",
    "training_labels = []\n",
    "\n",
    "testing_sentences = []\n",
    "testing_labels = []\n",
    "\n",
    "# Get training data from tensors\n",
    "for s, l in train_data:\n",
    "    training_sentences.append(str(s.numpy()))\n",
    "    training_labels.append(l.numpy())\n",
    "\n",
    "# Get test data from tensors\n",
    "for s, l in test_data:\n",
    "    testing_sentences.append(str(s.numpy()))\n",
    "    testing_labels.append(l.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert arrays to numpy arrays\n",
    "training_labels_final = np.array(training_labels)\n",
    "testing_labels_final = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify hyper-parameters\n",
    "vocab_size = 10000\n",
    "embedding_size = 16\n",
    "max_length = 120\n",
    "trunc_type = \"post\"\n",
    "oov_tok = \"<OOV>\"\n",
    "\n",
    "# Create a Tokenizer\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "\n",
    "# Create a word index\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Convert sentences to sequences\n",
    "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_length, truncating=trunc_type)\n",
    "\n",
    "# Use same tokenizer in test data - a lot more OOVs might be created\n",
    "testing_sentences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sentences, maxlen=max_length, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? ? ? ? ? ? ? b'i have been known to fall asleep during films but this is usually due to a combination of things including really tired being warm and comfortable on the <OOV> and having just eaten a lot however on this occasion i fell asleep because the film was rubbish the plot development was constant constantly slow and boring things seemed to happen but with no explanation of what was causing them or why i admit i may have missed part of the film but i watched the majority of it and everything just seemed to happen of its own <OOV> without any real concern for anything else i cant recommend this film at all '\n",
      "b'I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.'\n"
     ]
    }
   ],
   "source": [
    "# Reverse word index\n",
    "reverse_word_index = dict([(value, key) for key, value in word_index.items()])\n",
    "\n",
    "\n",
    "def decode_review(text):\n",
    "    return \" \".join([reverse_word_index.get(i, \"?\") for i in text])\n",
    "\n",
    "\n",
    "# Examine an example\n",
    "print(decode_review(training_padded[1]))\n",
    "print(training_sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Sequential Neural Network\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_size, input_length=max_length),\n",
    "    tf.keras.layers.Flatten(),    # Might give a very long vector\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 120, 16)           160000    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 11526     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 171,533\n",
      "Trainable params: 171,533\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Sequential Neural Network\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_size, input_length=max_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),  # Averages across the vector to flatten it out\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 120, 16)           160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 102       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 160,109\n",
      "Trainable params: 160,109\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.5817 - accuracy: 0.7380 - val_loss: 0.4493 - val_accuracy: 0.8029\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.3536 - accuracy: 0.8574 - val_loss: 0.3745 - val_accuracy: 0.8344\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.2851 - accuracy: 0.8854 - val_loss: 0.3776 - val_accuracy: 0.8316\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.2483 - accuracy: 0.9031 - val_loss: 0.3905 - val_accuracy: 0.8282\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.2216 - accuracy: 0.9161 - val_loss: 0.4065 - val_accuracy: 0.8262\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.2008 - accuracy: 0.9260 - val_loss: 0.4180 - val_accuracy: 0.8276\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1831 - accuracy: 0.9346 - val_loss: 0.4585 - val_accuracy: 0.8180\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1681 - accuracy: 0.9417 - val_loss: 0.4737 - val_accuracy: 0.8200\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1554 - accuracy: 0.9465 - val_loss: 0.5252 - val_accuracy: 0.8102\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1431 - accuracy: 0.9529 - val_loss: 0.5479 - val_accuracy: 0.8097\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x147b429f940>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model\n",
    "num_epochs = 10\n",
    "model.fit(training_padded, training_labels_final, epochs=num_epochs,\n",
    "          validation_data=(testing_padded, testing_labels_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentence = [\"I absolutely hated that movie\"]\n",
    "new_sentence = [\"I absolutely loved that movie\"]\n",
    "new_sentence = [\"What a wonderful film. I loved it\"]\n",
    "\n",
    "test_sentence = tokenizer.texts_to_sequences(new_sentence)\n",
    "padded_sentence = pad_sequences(test_sentence, maxlen=max_length, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive review\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(padded_sentence)\n",
    "if prediction > 0.5:\n",
    "    print(\"Positive review\")\n",
    "else:\n",
    "    print(\"Negative review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 16)\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for embeddings projector\n",
    "\n",
    "e = model.layers[0] # Take output of embedding (layer 0)\n",
    "weights = e.get_weights()[0] # Get the weights for that layer\n",
    "print(weights.shape)  # 10000 words, 16 dimensions\n",
    "\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for word_num in range(1, vocab_size):\n",
    "    word = reverse_word_index[word_num]\n",
    "    embeddings = weights[word_num]\n",
    "\n",
    "    out_m.write(word + \"\\n\")\n",
    "    out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
    "\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
